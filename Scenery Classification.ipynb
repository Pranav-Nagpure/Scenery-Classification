{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenery Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters\n",
    "\n",
    "# Batch Size for all Datasets\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Maximum number of Epochs for Training\n",
    "EPOCHS = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras.layers as layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from matplotlib.image import imread\n",
    "from IPython.display import clear_output\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Dataset\n",
    "!mkdir dataset\n",
    "%cd dataset\n",
    "!kaggle datasets download -d puneet6060/intel-image-classification\n",
    "clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip Dataset\n",
    "!tar -xf intel-image-classification.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Unwanted Files\n",
    "!del intel-image-classification.zip\n",
    "!rmdir /s/q seg_pred\n",
    "%cd ..\n",
    "clear_output()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading FilePaths in Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filepaths_to_dataframe(path):\n",
    "    labels = []\n",
    "    file_names = []\n",
    "    directories = os.listdir(path)\n",
    "    for directory in directories:\n",
    "        files = os.listdir(f'{path}/{directory}')\n",
    "        for file in files:\n",
    "            labels.append(directory)\n",
    "            file_names.append(f'{path}/{directory}/{file}')\n",
    "\n",
    "    labels = pd.Series(data=labels, name='label')\n",
    "    file_names = pd.Series(data=file_names, name='file_name')\n",
    "\n",
    "    return pd.concat([labels, file_names], axis=1)\n",
    "\n",
    "\n",
    "train_val_df = filepaths_to_dataframe('dataset/seg_train/seg_train')\n",
    "test_df = filepaths_to_dataframe('dataset/seg_test/seg_test')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vizualizing Some Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualize some random images\n",
    "images = train_val_df.sample(n=9)\n",
    "fig, ax = plt.subplots(3, 3, figsize=(15, 9))\n",
    "fig.suptitle('Some Random Image Samples')\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        k = 3 * i + j\n",
    "        img = imread(images.iloc[k, 1])\n",
    "        ax[i][j].imshow(img)\n",
    "        ax[i][j].set_title(images.iloc[k, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualize counts of classes\n",
    "data = train_val_df['label'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 2))\n",
    "ax.set_xlabel('Classes')\n",
    "ax.set_ylabel('Samples')\n",
    "ax.set_title('Image Samples in each Class')\n",
    "ax.bar(data.index, data)\n",
    "ax.tick_params(axis='x', labelrotation=90)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Up Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "encoder = LabelEncoder().fit(train_val_df['label'].unique())\n",
    "train_val_df['label'] = encoder.transform(train_val_df['label'])\n",
    "test_df['label'] = encoder.transform(test_df['label'])\n",
    "\n",
    "# Saving encoder data for Web Application\n",
    "enc_file = open('encoder_data.txt', 'w')\n",
    "for cls in encoder.classes_:\n",
    "    enc_file.write(cls+'\\n')\n",
    "enc_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting training dataframe into training and validation dataset\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Dataset\n",
    "def dataframe_to_dataset(df):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((df['file_name'], df['label']))\n",
    "    ds = ds.map(lambda x, y: (tf.io.read_file(x), y), num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "    ds = ds.map(lambda x, y: (tf.image.decode_jpeg(x, channels=3), y), num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "    ds = ds.map(lambda x, y: (tf.image.resize(x, (150, 150)), y), num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "    ds = ds.batch(batch_size=BATCH_SIZE, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "\n",
    "    # Optimize Performance\n",
    "    ds = ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "train_ds = dataframe_to_dataset(train_df)\n",
    "val_ds = dataframe_to_dataset(val_df)\n",
    "test_ds = dataframe_to_dataset(test_df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Layers\n",
    "scaling_layer = layers.Rescaling(1.0/255, input_shape=(150, 150, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "model = Sequential([scaling_layer,\n",
    "                    layers.Conv2D(16, kernel_size=(3, 3)),\n",
    "                    layers.BatchNormalization(),\n",
    "                    layers.LeakyReLU(),\n",
    "\n",
    "                    layers.Conv2D(32, kernel_size=(3, 3)),\n",
    "                    layers.BatchNormalization(),\n",
    "                    layers.LeakyReLU(),\n",
    "                    layers.MaxPooling2D(5, 5),\n",
    "\n",
    "                    layers.Conv2D(64, kernel_size=(3, 3)),\n",
    "                    layers.BatchNormalization(),\n",
    "                    layers.LeakyReLU(),\n",
    "\n",
    "                    layers.Conv2D(128, kernel_size=(3, 3)),\n",
    "                    layers.BatchNormalization(),\n",
    "                    layers.LeakyReLU(),\n",
    "                    layers.MaxPooling2D(5, 5),\n",
    "\n",
    "                    layers.Flatten(),\n",
    "\n",
    "                    layers.Dense(64),\n",
    "                    layers.Dropout(rate=0.2),\n",
    "                    layers.BatchNormalization(),\n",
    "                    layers.LeakyReLU(),\n",
    "\n",
    "                    layers.Dense(32),\n",
    "                    layers.Dropout(rate=0.2),\n",
    "                    layers.BatchNormalization(),\n",
    "                    layers.LeakyReLU(),\n",
    "\n",
    "                    layers.Dense(16),\n",
    "                    layers.Dropout(rate=0.2),\n",
    "                    layers.BatchNormalization(),\n",
    "                    layers.LeakyReLU(),\n",
    "\n",
    "                    layers.Dense(6, activation='softmax')])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Model\n",
    "history = model.fit(train_ds, epochs=EPOCHS, validation_data=val_ds, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training history\n",
    "fig, ax = plt.subplots(figsize=(20, 5))\n",
    "ax.set_title('Model Accuracy')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.plot(range(1, len(history.history['accuracy'])+1), history.history['accuracy'], label='train')\n",
    "ax.plot(range(1, len(history.history['val_accuracy'])+1), history.history['val_accuracy'], label='val')\n",
    "_ = ax.legend()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Accuracy on Test Data\n",
    "score = model.evaluate(test_ds)\n",
    "print(f'Accuracy over the test set: {round((score[1]*100), 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scenery",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
